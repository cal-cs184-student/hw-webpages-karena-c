<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Karena Chen, Steffi Lin </div>

		<br>

		Link to webpage: <a href="https://cal-cs184-student.github.io/hw-webpages-karena-c/hw3/index.html">https://cal-cs184-student.github.io/hw-webpages-karena-c/hw3/index.html</a><br>
		Link to GitHub repository: <a href="https://github.com/cal-cs184-student/sp25-hw3-matcha_latte/tree/master">https://github.com/cal-cs184-student/sp25-hw3-matcha_latte</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>

		<p><b>Ray Generation:</b> We first define the camera space as the bottom left corner located at (-tan(hFov/2), -tan(vFov/2), -1), the top right corner located at (tan(hFov/2), tan(vFov/2), -1), and the center of the sensor at (0, 0, 1) using the formulas given by the spec. As the given image coordinates (x, y) are normalized, we can determine what the point on the sensor corresponds to the image coordinates and find a ray based on the camera position pos. Then, we transform the ray from camera space to world space using the c2w matrix and update min_t and max_t with nclip and fclip respectively.</p>
		<p><b>Pixel Sample Generation:</b> To generate pixel samples, we iterate over the number of samples given to us by ns_aa. For each iteration, we generate a random ray using the camera->generate_ray function and estimate scene radiance for that ray using est_radiance_global_illumination, averaging out the value over all the samples and updating the buffer.</p>

		<p><b>Ray-Triangle Intersection:</b> We used the Möller Trumbore algorithm described in lecture to determine whether there is an intersection between the ray and the triangle. It mathematically calculates the variables t, b1, and b2 to solve for any intersections between the ray and triangle. If t, b1, and b2 satisfy all the conditions, they represent a valid-ray triangle intersection, and we update the new max_t and intersection.</p>

		<p><b>Ray-Sphere Intersection:</b> Similar to the ray-triangle, we use the formula described in lecture to solve for all instances of intersections at t, and use the one that is closest and still valid between min_t and max_t. We update max_t and the intersection accordingly.</p>
		<div></div>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBempty.png" width="400px"/>
				  <figcaption>CBempty.png</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBspheres.png" width="400px"/>
				  <figcaption>CBSpheres.png</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBgems.png" width="400px"/>
				  <figcaption>CBgems.png</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny.png" width="400px"/>
				  <figcaption>CBbunny.png</figcaption>
				</td>
			  </tr>
			</table>
		</div>
		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<p>We first computed a bounding box for all the current primitives, then we created a BVHNode with this bounding box. If the number of primitives is small enough (<= max_leaf_size), we made this node a leaf node by setting start and end and making the left and right children null pointers. If we aren’t creating a leaf node, we split the primitives by first calculating the centroid bounding box, and determining the axis with the largest extent. We then calculated the midpoint along this axis and partitioned primitives into two groups based on whether their centroid is less than this midpoint. If everything ends up on one side, we fallback to the simple median split. We then recursively construct the left and right child nodes. </p>
		<p>When comparing rendering times with and without BVH acceleration, we noticed a very significant performance difference. Without BVH, each ray is tested for an intersection between every primitive. By using BVH, we significantly decrease the number of tests and therefore significantly improve the runtime. For some scenes like the maxplanck.png, the runtime was improved from a few minutes to under a second. For cow.dae, we improved our render time from around 5 seconds to 0.03 seconds. In scenes before implementing acceleration, the average number of intersections per ray was also significantly higher than with BVH. We went from around 1500 intersections per ray on some renders to an average of 0 intersections per ray. </p>

		<div></div>
		<h2>Part 3: Direct Illumination</h2>
		<p><b>DiffuseBSDF::f:</b> Since light is being reflected equally in all directions on the hemisphere, we return reflectance/pi.</p>
		<p><b>Zero-bounce Illumination:</b> We returned the emission of the object that was intersected using the function bsdf->get_emission().</p>

		<div></div>

		<p><b>Estimate_Direct_Lighting_Hemisphere:</b> For estimate_direct_lighting_hemisphere, we use hemisphereSampler->get_sample() to get a uniformly and randomly sampled vector on the hemisphere. We then check to see if there is an intersection at that ray using bvh->intersect, and get the approximate amount of light using the Monte Carlo estimator. f is our calculated bsdf, l is the light given by the emission, and cos_theta is the dot product of the surface normal and our sampled ray in world. Since we are uniformly sampling across the hemisphere, our pdf is 1/2pi. We sample for a total number of num_samples, and find the average.</p>
		<p><b>Estimate_Direct_Lighting_Importance:</b> For estimate_direct_lighting_importance, we sample across each light in the scene. We check if the light is a point light source, only sampling once if it is one to save time, and sampling ns_area_light number of times otherwise. We check if the light source casts light onto the object by checking whether or not there is an intersection, and return the average of all contributed light using the Monte Carlo estimator.</p>

		<div></div>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/CBbunny_H_16_8.png" width="400px"/>
				  <figcaption>Uniform Hemisphere Sampling Low Res</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/CBbunny_H_64_32.png" width="400px"/>
				  <figcaption>Uniform Hemisphere Sampling High Res</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/bunny_1_1.png" width="400px"/>
				  <figcaption>Importance Sampling Low Res</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/bunny_64_32.png" width="400px"/>
				  <figcaption>Importance Sampling High Res</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/dragon_64_32.png" width="400px"/>
				  <figcaption>Importance Sampling Dragon</figcaption>
				</td>
			  </tr>	
			</table>
		</div>

		<p>Uniform hemisphere sampling has a lot more noise overall compared to lighting sampling. The noise generated by uniform hemisphere sampling is also very uniform across the entire image, which is due to how the sampling is implemented to assume lighting is coming from all directions. With lighting samples, the image comes across a lot cleaner, with very little noise. As we are sampling directly from the light sources, the images created are smoother and cleaner.</p>

		<div></div>
		<div style="display: flex; flex-direction: column; align-items: center;">
			<table style="width: 100%; text-align: center; border-collapse: collapse;">
			  <tr>
				<td style="text-align: center;">
				  <img src="images/q3Bunny_1_1.png" width="400px"/>
				  <figcaption>s = 1, l = 1</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/q3Bunny_1_4.png" width="400px"/>
				  <figcaption>s = 1, l = 4</figcaption>
				</td>
			  </tr>
			  <tr>
				<td style="text-align: center;">
				  <img src="images/q3Bunny_1_16.png" width="400px"/>
				  <figcaption>s = 1, l = 16</figcaption>
				</td>
				<td style="text-align: center;">
				  <img src="images/q3Bunny_1_64" width="400px"/>
				  <figcaption>s = 1, l = 64</figcaption>
				</td>
			  </tr>
			</table>
		</div>

		<p>As the number of light rays increases, the noise level disappears. We can see this especially in the bunny’s shadow, where with each increase in the number of light rays, the averaged light illumination can be better estimated, and results in a more gradual change in light and thereby decreases the noise.</p>
			
		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>
